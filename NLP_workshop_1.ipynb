{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Workshop Part-1\n",
    "### Text as Data\n",
    "\n",
    "---\n",
    "\n",
    "Text is an extremely rich source for data. Tapping into it can yield great insights, but creating a pipeline from raw data to metrics and visualizations often feels like it requires more grit and luck than anything else.\n",
    "\n",
    "In this first workshop we'll take a high-level look at some of the fundamentals of Natural Language Processing, or NLP, from cleaning data to extracting insights.\n",
    "\n",
    "---\n",
    "\n",
    "**Contents:**\n",
    "* Cleaning/Preprocessing Text (80 - 95%)\n",
    "    - tokenization, removal (punctuation, stopwords), stemming/lemmatization\n",
    "* Analyzing Text (5 - 20%)\n",
    "    - word clouds, tfidf, clustering\n",
    "\n",
    "---\n",
    "\n",
    "# Phase 1: initial approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example install, un-comment the line below to run\n",
    "\n",
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data storage\n",
    "import pickle\n",
    "\n",
    "# elemental libraries for text data\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# fancy NLP libraries\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# clustering imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# visuals\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is copy paste from a google search of making a wordcloud\n",
    "\n",
    "def make_cloud(freq_dict):\n",
    "    '''make a wordcloud from a word frequency dictionary'''\n",
    "    \n",
    "    wordcloud = WordCloud()\n",
    "    wordcloud.generate_from_frequencies(frequencies=freq_dict)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in pickled data\n",
    "\n",
    "job_text = pickle.load(open('job_descriptions.pkl', 'rb'))\n",
    "len(job_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first job description\n",
    "\n",
    "example_text = job_text[0]\n",
    "\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokens are the elemental - or indivisible - pieces of a text document. We can break text into words, n-grams, and sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens generated from single words\n",
    "\n",
    "tokens = word_tokenize(example_text)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Counter to count words\n",
    "\n",
    "Counter(tokens).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look\n",
    "\n",
    "make_cloud(Counter(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using \"tokens\" we can call ngrams to get chunks of words as tokens\n",
    "\n",
    "list(ngrams(tokens[:20], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the words within each token, and calling wordcloud\n",
    "\n",
    "bigrams = [' '.join(gram) for gram in ngrams(tokens, 2)]\n",
    "\n",
    "make_cloud(Counter(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing sentences is tricky\n",
    "\n",
    "sent_tokenize(example_text)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 2: Cleaning Up\n",
    "\n",
    "Know how to clean your data.\n",
    "\n",
    "# Cleaning Up - identifying and removing stuff\n",
    "\n",
    "Something something, data science is art. This step will always be subject to your data, your business problem, and politics. \n",
    "\n",
    "\"Knowing what you can and can't do makes you a great analyst, knowing what you should and shouldn't do makes you the manager of analysts.\"\n",
    "\n",
    "-Dr. Hugh Watson, probably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check every character in our document\n",
    "\n",
    "unique_characters = {i for i in example_text}\n",
    "unique_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# characters in string.punctuation\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing everything but letters\n",
    "# also, moving to lower-case only\n",
    "\n",
    "letters = {i.lower() for i in unique_characters if i not in string.punctuation + string.digits}\n",
    "letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing all the non-letters\n",
    "\n",
    "really_only_letters = {i for i in letters if i not in ['\\n', '’', '“', '”']}\n",
    "really_only_letters  # and spaces, because a blob of text without spaces would be even worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bringing it all together, let's make our text into only text\n",
    "\n",
    "non_char = string.punctuation + string.digits + '\\n' + '’' + '“' + '”'\n",
    "\n",
    "cleaner_text = re.sub('[%s]' % re.escape(non_char), ' ', example_text.lower())\n",
    "cleaner_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Up - stop words\n",
    "\n",
    "Stop words: common words like \"and, this, that\" which have little to no value for semantic analysis.\n",
    "\n",
    "Removing stop words removes noise, enhancing relevant information\n",
    "\n",
    "\"The dog ran really fast.\" -> \"dog ran fast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some stop words\n",
    "\n",
    "sorted(stopwords.words('english'))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply remove the stopwords\n",
    "# note: using split() and join together also removes extra spaces\n",
    "\n",
    "even_cleaner_text = ' '.join([i for i in cleaner_text.split() \n",
    "                              if i not in stopwords.words('english')])\n",
    "even_cleaner_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Up - stemming and lemmatization\n",
    "\n",
    "Stemming: reducing words to base form\n",
    "\n",
    "Lemmatization: stemming + crazy linguistics stuff\n",
    "\n",
    "Essentially, the goal here is to make our lexicon significantly smaller - i.e. reduce complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick example\n",
    "\n",
    "stemmer = PorterStemmer()  # stemming class\n",
    "lemmer = WordNetLemmatizer()  # lemmatization class\n",
    "\n",
    "same_things = ['ran', 'run', 'runs', 'running', 'runner', 'runners']\n",
    "\n",
    "for i in same_things:\n",
    "    print('Stem - {}  ;  Lemma - {}'.format(stemmer.stem(i), lemmer.lemmatize(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squeaky clean\n",
    "\n",
    "cleanest_text = ' '.join([lemmer.lemmatize(i) for i in even_cleaner_text.split()])\n",
    "cleanest_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time for another cloud\n",
    "\n",
    "tokens = word_tokenize(cleanest_text)\n",
    "bigrams = [' '.join(gram) for gram in ngrams(tokens, 2)]\n",
    "\n",
    "make_cloud(Counter(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_cloud(Counter(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling all of the prior stuff into a function\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    given some text, clean it\n",
    "    import string\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk import WordNetLemmatizer\n",
    "    '''\n",
    "    # remove stuff\n",
    "    non_char = string.punctuation + string.digits + '\\n' + '’' + '“' + '”'\n",
    "    cleaner_text = re.sub('[%s]' % re.escape(non_char), ' ', text.lower())\n",
    "    # import lemma\n",
    "    lemma = WordNetLemmatizer()\n",
    "    # combine lemma and stopword removal\n",
    "    clean_text = ' '.join([lemma.lemmatize(i) for i in cleaner_text.split() \n",
    "                           if i not in stopwords.words('english')])\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "\n",
    "# clean all text\n",
    "\n",
    "clean_jobs = [clean(i) for i in job_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase 3: Analysis\n",
    "\n",
    "# Analysis - CountVectorizer\n",
    "\n",
    "It's easy enough to apply the above to all of our text with a function, but we need some way to manipulate it all as data. Counter is sufficient for one document, CountVectorizer is sufficient for many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spin up CountVectorizer and test it out on the cleanest text\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "X = cv.fit_transform([cleanest_text])\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV has ngrams builtin\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "X = cv.fit_transform([cleanest_text])\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now run it on everything\n",
    "\n",
    "cv = CountVectorizer(min_df=0.1, max_df=0.9)\n",
    "\n",
    "X = cv.fit_transform(clean_jobs)\n",
    "counts = pd.DataFrame(X.toarray(), columns=cv.get_feature_names())\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_cloud(counts.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2), min_df=0.1, max_df=0.9)\n",
    "\n",
    "X = cv.fit_transform(clean_jobs)\n",
    "bigram_counts = pd.DataFrame(X.toarray(), columns=cv.get_feature_names())\n",
    "\n",
    "make_cloud(bigram_counts.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - TFIDF\n",
    "\n",
    "Counts are a good source of information, but Term Frequency Inverse Document Frequency gives a weight representation of word importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# straight to the df\n",
    "\n",
    "tf = TfidfVectorizer(ngram_range=(1, 2), min_df=0.1, max_df=0.9)\n",
    "\n",
    "X = tf.fit_transform(clean_jobs)\n",
    "scores = pd.DataFrame(X.toarray(), columns=tf.get_feature_names())\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce feature-space and cluster\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(scores)\n",
    "pca_scores = pca.transform(scores)\n",
    "\n",
    "k = 3\n",
    "km = KMeans(n_clusters=k, random_state=42)\n",
    "km.fit(pca_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster labels\n",
    "\n",
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clouds of clusters\n",
    "\n",
    "scores['cluster_label'] = km.labels_\n",
    "\n",
    "for i in range(k):\n",
    "    cluster = scores[scores['cluster_label'] == i].iloc[:,:-1]\n",
    "    make_cloud(cluster.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
